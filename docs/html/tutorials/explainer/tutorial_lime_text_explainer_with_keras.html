

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Contextual AI LIME Text ExplainerFactory with Keras &mdash; Contextual AI  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="SHAP Kernel Explainer for Tabular Data via Contextual AI" href="tutorial_shap_tabular_explainer.html" />
    <link rel="prev" title="LIME Text Explainer via XAI" href="tutorial_lime_text_explainer.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Contextual AI
          

          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contextual AI Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../data_module.html">Data Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training_module.html">Model Module</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../inference_module.html">Explainer Module</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../inference_module_tutorial.html">Tutorial</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../inference_module_tutorial.html#tutorial">Tutorial</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="tutorial_lime_tabular_explainer.html">LIME Tabular Explainer via XAI</a></li>
<li class="toctree-l4"><a class="reference internal" href="tutorial_lime_text_explainer.html">LIME Text Explainer via XAI</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Contextual AI LIME Text ExplainerFactory with Keras</a></li>
<li class="toctree-l4"><a class="reference internal" href="tutorial_shap_tabular_explainer.html">SHAP Kernel Explainer for Tabular Data via Contextual AI</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../explainer/explainer.html">API Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../formatter_module.html">Formatter Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler_module.html">Compiler Module</a></li>
</ul>
<p class="caption"><span class="caption-text">Miscellaneous</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autodoc.html">Contextual AI Auto-doc</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Contextual AI</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../inference_module.html">Explainer Module</a> &raquo;</li>
        
          <li><a href="../../inference_module_tutorial.html">Contextual AI Explainer tutorials</a> &raquo;</li>
        
      <li>Contextual AI LIME Text ExplainerFactory with Keras</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/tutorials/explainer/tutorial_lime_text_explainer_with_keras.nblink.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Contextual-AI-LIME-Text-ExplainerFactory-with-Keras">
<h1>Contextual AI LIME Text ExplainerFactory with Keras<a class="headerlink" href="#Contextual-AI-LIME-Text-ExplainerFactory-with-Keras" title="Permalink to this headline">¶</a></h1>
<p>This tutorial is similar to <a class="reference external" href="lime_text_explainer.ipynb">lime_text_explainer.ipynb</a>, but instead of a Naive Bayes model we attempt to generate explanations with a neural network implemented with Keras.</p>
<p>The neural network is a simple multi-layer CNN with GloVe embeddings. This tutorial requires you to download the pre-trained word embeddings from this <a class="reference external" href="http://nlp.stanford.edu/data/glove.6B.zip">link</a> (caution - this link initiates a 822MB download).</p>
<p>The modelling/text processing portions of this tutorial are heavily borrowed from <a class="reference external" href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">this Keras blog</a>.</p>
<p>Like with other Contextual AI tutorials, the main steps for generating explanations are:</p>
<ol class="arabic simple">
<li><p>Get an explainer via the <code class="docutils literal notranslate"><span class="pre">ExplainerFactory</span></code> class</p></li>
<li><p>Build the text explainer</p></li>
<li><p>Call <code class="docutils literal notranslate"><span class="pre">explain_instance</span></code></p></li>
</ol>
<div class="section" id="Step-1:-Import-libraries">
<h2>Step 1: Import libraries<a class="headerlink" href="#Step-1:-Import-libraries" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Some auxiliary imports for the tutorial</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> \
<span class="n">Conv1D</span><span class="p">,</span> <span class="n">MaxPooling1D</span><span class="p">,</span> <span class="n">Convolution1D</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">Concatenate</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">keras.initializers</span> <span class="kn">import</span> <span class="n">Constant</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib.learn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">class_weight</span>

<span class="c1"># Set seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123456</span><span class="p">)</span>

<span class="c1"># Set the path so that we can import the ExplainerFactory</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;../../&#39;</span><span class="p">)</span>

<span class="c1"># Main Contextual AI imports</span>
<span class="kn">import</span> <span class="nn">xai</span>
<span class="kn">from</span> <span class="nn">xai.explainer</span> <span class="kn">import</span> <span class="n">ExplainerFactory</span>

<span class="c1">###################################################</span>
<span class="c1"># Set the directory to the GloVe embeddings here! #</span>
<span class="c1">###################################################</span>
<span class="n">GLOVE_DIR</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
<span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">MAX_NUM_WORDS</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">VALIDATION_SPLIT</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">HIDDEN_UNITS</span> <span class="o">=</span> <span class="mi">128</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Using TensorFlow backend.
</pre></div></div>
</div>
</div>
<div class="section" id="Step-2:-Load-dataset-and-train-a-model">
<h2>Step 2: Load dataset and train a model<a class="headerlink" href="#Step-2:-Load-dataset-and-train-a-model" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we rely on the 20newsgroups text dataset, which can be loaded via sklearn’s dataset utility. Documentation on the dataset itself can be found <a class="reference external" href="https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html">here</a>. To keep things simple, we will extract data for 3 topics - baseball, Christianity, and medicine.</p>
<p>Our target model is a CNN which ingest pre-trained word embeddings.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Train on a subset of categories</span>

<span class="n">categories</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;rec.sport.baseball&#39;</span><span class="p">,</span>
    <span class="s1">&#39;soc.religion.christian&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sci.med&#39;</span>
<span class="p">]</span>

<span class="n">raw_train</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">raw_train</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">raw_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">raw_train</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="n">raw_test</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">)</span>

<span class="c1"># Turn text into lowercase</span>
<span class="n">raw_train_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">raw_train</span><span class="o">.</span><span class="n">data</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">raw_train</span><span class="o">.</span><span class="n">target</span>
<span class="n">raw_test_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">raw_test</span><span class="o">.</span><span class="n">data</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">raw_test</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">char_level</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="s1">&#39;UNK&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">raw_train_text</span><span class="p">)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>

<span class="c1"># Convert string to index</span>
<span class="n">train_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">raw_train_text</span><span class="p">)</span>
<span class="n">test_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">raw_test_text</span><span class="p">)</span>

<span class="c1"># Padding</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">train_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">test_texts</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">)</span>

<span class="c1"># Convert to numpy array</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">y_train_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">y_valid_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">y_test_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;DESCR&#39;, &#39;target&#39;, &#39;data&#39;, &#39;filenames&#39;, &#39;target_names&#39;]
[&#39;rec.sport.baseball&#39;, &#39;sci.med&#39;, &#39;soc.religion.christian&#39;]
[1 0 2 2 0 2 0 0 0 1]
</pre></div></div>
</div>
</div>
<div class="section" id="Prepare-the-embedding-matrix">
<h2>Prepare the embedding matrix<a class="headerlink" href="#Prepare-the-embedding-matrix" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Prepare the embedding matrix</span>
<span class="c1"># Code comes from https://keras.io/examples/pretrained_word_embeddings/</span>
<span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">GLOVE_DIR</span><span class="p">,</span> <span class="s1">&#39;glove.6B.100d.txt&#39;</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
        <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>

<span class="c1"># prepare embedding matrix</span>
<span class="n">num_words</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">MAX_NUM_WORDS</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_words</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">))</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">MAX_NUM_WORDS</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># words not found in embedding index will be all-zeros.</span>
        <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>

<span class="c1"># load pre-trained word embeddings into an Embedding layer</span>
<span class="c1"># note that we set trainable = False so as to keep the embeddings fixed</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">num_words</span><span class="p">,</span>
                            <span class="n">EMBEDDING_DIM</span><span class="p">,</span>
                            <span class="n">embeddings_initializer</span><span class="o">=</span><span class="n">Constant</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">),</span>
                            <span class="n">input_length</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,</span>
                            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-the-model">
<h2>Define the model<a class="headerlink" href="#Define-the-model" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Prepare the model</span>

<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="mi">4</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="mi">4</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">y_train_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 1000)              0
_________________________________________________________________
embedding_1 (Embedding)      (None, 1000, 100)         8100
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 996, 128)          64128
_________________________________________________________________
batch_normalization_1 (Batch (None, 996, 128)          512
_________________________________________________________________
activation_1 (Activation)    (None, 996, 128)          0
_________________________________________________________________
dropout_1 (Dropout)          (None, 996, 128)          0
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 249, 128)          0
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 245, 128)          82048
_________________________________________________________________
batch_normalization_2 (Batch (None, 245, 128)          512
_________________________________________________________________
activation_2 (Activation)    (None, 245, 128)          0
_________________________________________________________________
dropout_2 (Dropout)          (None, 245, 128)          0
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 61, 128)           0
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 57, 128)           82048
_________________________________________________________________
batch_normalization_3 (Batch (None, 57, 128)           512
_________________________________________________________________
activation_3 (Activation)    (None, 57, 128)           0
_________________________________________________________________
dropout_3 (Dropout)          (None, 57, 128)           0
_________________________________________________________________
flatten_1 (Flatten)          (None, 7296)              0
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 21891
=================================================================
Total params: 259,751
Trainable params: 250,883
Non-trainable params: 8,868
_________________________________________________________________
</pre></div></div>
</div>
</div>
<div class="section" id="Train-the-model">
<h2>Train the model<a class="headerlink" href="#Train-the-model" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X_train</span><span class="p">],</span> <span class="n">y_train_onehot</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
          <span class="n">validation_data</span><span class="o">=</span><span class="p">([</span><span class="n">X_valid</span><span class="p">],</span> <span class="n">y_valid_onehot</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train on 1432 samples, validate on 358 samples
Epoch 1/100
1432/1432 [==============================] - 5s 3ms/step - loss: 2.1773 - acc: 0.3897 - val_loss: 2.4187 - val_acc: 0.3743
Epoch 2/100
1432/1432 [==============================] - 0s 264us/step - loss: 1.1169 - acc: 0.5740 - val_loss: 1.2449 - val_acc: 0.4749
Epoch 3/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.7730 - acc: 0.6899 - val_loss: 0.7144 - val_acc: 0.6732
Epoch 4/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.4197 - acc: 0.8156 - val_loss: 3.1116 - val_acc: 0.3575
Epoch 5/100
1432/1432 [==============================] - 0s 220us/step - loss: 0.2677 - acc: 0.8897 - val_loss: 0.5152 - val_acc: 0.7821
Epoch 6/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.2428 - acc: 0.9064 - val_loss: 0.9468 - val_acc: 0.6732
Epoch 7/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.1793 - acc: 0.9385 - val_loss: 3.4665 - val_acc: 0.5754
Epoch 8/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.1346 - acc: 0.9497 - val_loss: 3.2727 - val_acc: 0.3994
Epoch 9/100
1432/1432 [==============================] - 0s 220us/step - loss: 0.0958 - acc: 0.9728 - val_loss: 6.0987 - val_acc: 0.3855
Epoch 10/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0677 - acc: 0.9777 - val_loss: 4.9628 - val_acc: 0.3939
Epoch 11/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0732 - acc: 0.9742 - val_loss: 3.5400 - val_acc: 0.4302
Epoch 12/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0802 - acc: 0.9728 - val_loss: 1.5637 - val_acc: 0.6061
Epoch 13/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.1385 - acc: 0.9567 - val_loss: 8.2588 - val_acc: 0.3575
Epoch 14/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0578 - acc: 0.9818 - val_loss: 4.9207 - val_acc: 0.6117
Epoch 15/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0652 - acc: 0.9825 - val_loss: 3.7316 - val_acc: 0.5615
Epoch 16/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0436 - acc: 0.9853 - val_loss: 1.5448 - val_acc: 0.6508
Epoch 17/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0824 - acc: 0.9804 - val_loss: 5.0774 - val_acc: 0.6006
Epoch 18/100
1432/1432 [==============================] - 0s 220us/step - loss: 0.0448 - acc: 0.9853 - val_loss: 7.6370 - val_acc: 0.3855
Epoch 19/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0578 - acc: 0.9797 - val_loss: 4.4715 - val_acc: 0.4385
Epoch 20/100
1432/1432 [==============================] - 0s 220us/step - loss: 0.0439 - acc: 0.9846 - val_loss: 5.1295 - val_acc: 0.5084
Epoch 21/100
1432/1432 [==============================] - 0s 220us/step - loss: 0.0285 - acc: 0.9881 - val_loss: 7.6370 - val_acc: 0.3715
Epoch 22/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0509 - acc: 0.9846 - val_loss: 5.2909 - val_acc: 0.4972
Epoch 23/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0324 - acc: 0.9860 - val_loss: 3.9168 - val_acc: 0.5475
Epoch 24/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0345 - acc: 0.9909 - val_loss: 5.7225 - val_acc: 0.3966
Epoch 25/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0425 - acc: 0.9867 - val_loss: 2.1274 - val_acc: 0.6899
Epoch 26/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0276 - acc: 0.9881 - val_loss: 1.1043 - val_acc: 0.7514
Epoch 27/100
1432/1432 [==============================] - 0s 215us/step - loss: 0.0339 - acc: 0.9874 - val_loss: 2.7648 - val_acc: 0.5866
Epoch 28/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0335 - acc: 0.9881 - val_loss: 0.9610 - val_acc: 0.7905
Epoch 29/100
1432/1432 [==============================] - 0s 220us/step - loss: 0.0227 - acc: 0.9930 - val_loss: 5.7261 - val_acc: 0.5782
Epoch 30/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0163 - acc: 0.9944 - val_loss: 10.2775 - val_acc: 0.3436
Epoch 31/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0206 - acc: 0.9930 - val_loss: 1.2157 - val_acc: 0.7542
Epoch 32/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0239 - acc: 0.9923 - val_loss: 5.4944 - val_acc: 0.4385
Epoch 33/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0108 - acc: 0.9951 - val_loss: 1.3424 - val_acc: 0.7905
Epoch 34/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0397 - acc: 0.9881 - val_loss: 4.4665 - val_acc: 0.5112
Epoch 35/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0212 - acc: 0.9923 - val_loss: 2.5467 - val_acc: 0.6788
Epoch 36/100
1432/1432 [==============================] - 0s 215us/step - loss: 0.0455 - acc: 0.9860 - val_loss: 5.6098 - val_acc: 0.4860
Epoch 37/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0227 - acc: 0.9930 - val_loss: 5.3960 - val_acc: 0.4553
Epoch 38/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0435 - acc: 0.9888 - val_loss: 7.5935 - val_acc: 0.3687
Epoch 39/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0309 - acc: 0.9902 - val_loss: 1.2254 - val_acc: 0.7514
Epoch 40/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0143 - acc: 0.9937 - val_loss: 1.7116 - val_acc: 0.6983
Epoch 41/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0425 - acc: 0.9895 - val_loss: 0.3884 - val_acc: 0.8883
Epoch 42/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0233 - acc: 0.9923 - val_loss: 1.0631 - val_acc: 0.7626
Epoch 43/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0224 - acc: 0.9944 - val_loss: 0.7163 - val_acc: 0.8575
Epoch 44/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0209 - acc: 0.9937 - val_loss: 2.8861 - val_acc: 0.6564
Epoch 45/100
1432/1432 [==============================] - 0s 215us/step - loss: 0.0717 - acc: 0.9818 - val_loss: 2.7609 - val_acc: 0.6955
Epoch 46/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0056 - acc: 0.9972 - val_loss: 0.9425 - val_acc: 0.8128
Epoch 47/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0185 - acc: 0.9958 - val_loss: 0.3938 - val_acc: 0.8966
Epoch 48/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0162 - acc: 0.9944 - val_loss: 0.8557 - val_acc: 0.8296
Epoch 49/100
1432/1432 [==============================] - 0s 215us/step - loss: 0.0204 - acc: 0.9916 - val_loss: 2.1519 - val_acc: 0.7123
Epoch 50/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 3.7780 - val_acc: 0.6313
Epoch 51/100
1432/1432 [==============================] - 0s 221us/step - loss: 0.0214 - acc: 0.9930 - val_loss: 3.6579 - val_acc: 0.6313
Epoch 52/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0263 - acc: 0.9916 - val_loss: 0.7616 - val_acc: 0.8268
Epoch 53/100
1432/1432 [==============================] - 0s 221us/step - loss: 0.0056 - acc: 0.9993 - val_loss: 4.4331 - val_acc: 0.5615
Epoch 54/100
1432/1432 [==============================] - 0s 215us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 3.3715 - val_acc: 0.6173
Epoch 55/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0337 - acc: 0.9888 - val_loss: 3.3990 - val_acc: 0.6480
Epoch 56/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0045 - acc: 0.9979 - val_loss: 7.0738 - val_acc: 0.4413
Epoch 57/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0191 - acc: 0.9951 - val_loss: 2.2512 - val_acc: 0.7151
Epoch 58/100
1432/1432 [==============================] - 0s 215us/step - loss: 0.0170 - acc: 0.9930 - val_loss: 2.9241 - val_acc: 0.6480
Epoch 59/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0213 - acc: 0.9916 - val_loss: 5.3108 - val_acc: 0.6089
Epoch 60/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.3068 - val_acc: 0.7961
Epoch 61/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0233 - acc: 0.9909 - val_loss: 1.2623 - val_acc: 0.7207
Epoch 62/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0146 - acc: 0.9944 - val_loss: 1.4777 - val_acc: 0.7402
Epoch 63/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0070 - acc: 0.9972 - val_loss: 4.0222 - val_acc: 0.5196
Epoch 64/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0075 - acc: 0.9979 - val_loss: 1.5516 - val_acc: 0.7709
Epoch 65/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0134 - acc: 0.9930 - val_loss: 1.0965 - val_acc: 0.7514
Epoch 66/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0249 - acc: 0.9909 - val_loss: 3.6485 - val_acc: 0.6480
Epoch 67/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0281 - acc: 0.9923 - val_loss: 2.7969 - val_acc: 0.5838
Epoch 68/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0053 - acc: 0.9986 - val_loss: 5.0894 - val_acc: 0.5503
Epoch 69/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0095 - acc: 0.9972 - val_loss: 3.8370 - val_acc: 0.5782
Epoch 70/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0107 - acc: 0.9972 - val_loss: 1.3765 - val_acc: 0.7877
Epoch 71/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0075 - acc: 0.9965 - val_loss: 2.2304 - val_acc: 0.7346
Epoch 72/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0134 - acc: 0.9979 - val_loss: 1.8926 - val_acc: 0.7179
Epoch 73/100
1432/1432 [==============================] - 0s 222us/step - loss: 0.0222 - acc: 0.9944 - val_loss: 3.0531 - val_acc: 0.6732
Epoch 74/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0099 - acc: 0.9958 - val_loss: 0.7451 - val_acc: 0.8324
Epoch 75/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0103 - acc: 0.9951 - val_loss: 4.3561 - val_acc: 0.5615
Epoch 76/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0070 - acc: 0.9965 - val_loss: 3.4871 - val_acc: 0.6844
Epoch 77/100
1432/1432 [==============================] - 0s 221us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 2.5512 - val_acc: 0.7291
Epoch 78/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0108 - acc: 0.9958 - val_loss: 1.6322 - val_acc: 0.7207
Epoch 79/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0177 - acc: 0.9937 - val_loss: 1.0522 - val_acc: 0.8101
Epoch 80/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0197 - acc: 0.9958 - val_loss: 1.4012 - val_acc: 0.7793
Epoch 81/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0280 - acc: 0.9916 - val_loss: 1.6188 - val_acc: 0.7486
Epoch 82/100
1432/1432 [==============================] - 0s 221us/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.7318 - val_acc: 0.8464
Epoch 83/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0033 - acc: 0.9986 - val_loss: 1.8894 - val_acc: 0.7514
Epoch 84/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0074 - acc: 0.9972 - val_loss: 0.8990 - val_acc: 0.8603
Epoch 85/100
1432/1432 [==============================] - 0s 219us/step - loss: 0.0202 - acc: 0.9951 - val_loss: 5.4534 - val_acc: 0.6117
Epoch 86/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0212 - acc: 0.9937 - val_loss: 4.7464 - val_acc: 0.6145
Epoch 87/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 6.7823 - val_acc: 0.4581
Epoch 88/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 9.6006 - val_acc: 0.3603
Epoch 89/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0094 - acc: 0.9986 - val_loss: 3.7251 - val_acc: 0.6816
Epoch 90/100
1432/1432 [==============================] - 0s 214us/step - loss: 0.0062 - acc: 0.9979 - val_loss: 3.1987 - val_acc: 0.6844
Epoch 91/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0092 - acc: 0.9979 - val_loss: 3.2621 - val_acc: 0.6760
Epoch 92/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0285 - acc: 0.9916 - val_loss: 7.5566 - val_acc: 0.3939
Epoch 93/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 3.0771 - val_acc: 0.5894
Epoch 94/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 2.3383 - val_acc: 0.7011
Epoch 95/100
1432/1432 [==============================] - 0s 218us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.5835 - val_acc: 0.8715
Epoch 96/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 1.6642 - val_acc: 0.7849
Epoch 97/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0074 - acc: 0.9986 - val_loss: 5.3298 - val_acc: 0.4665
Epoch 98/100
1432/1432 [==============================] - 0s 216us/step - loss: 0.0143 - acc: 0.9951 - val_loss: 2.5091 - val_acc: 0.6760
Epoch 99/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0180 - acc: 0.9937 - val_loss: 2.2953 - val_acc: 0.7095
Epoch 100/100
1432/1432 [==============================] - 0s 217us/step - loss: 0.0145 - acc: 0.9972 - val_loss: 0.4745 - val_acc: 0.8911
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;keras.callbacks.History at 0x7fb629d771d0&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-3:-Instantiate-the-explainer">
<h2>Step 3: Instantiate the explainer<a class="headerlink" href="#Step-3:-Instantiate-the-explainer" title="Permalink to this headline">¶</a></h2>
<p>Here, we will use the LIME Text ExplainerFactory.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">explainer</span> <span class="o">=</span> <span class="n">ExplainerFactory</span><span class="o">.</span><span class="n">get_explainer</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="n">xai</span><span class="o">.</span><span class="n">DOMAIN</span><span class="o">.</span><span class="n">TEXT</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-4:-Build-the-explainer">
<h2>Step 4: Build the explainer<a class="headerlink" href="#Step-4:-Build-the-explainer" title="Permalink to this headline">¶</a></h2>
<p>This initializes the underlying explainer object. We provide the <code class="docutils literal notranslate"><span class="pre">explain_instance</span></code> method below with the raw text - LIME’s text explainer algorithm will conduct its own preprocessing in order to generate interpretable representations of the data. Hence we must define a custom <code class="docutils literal notranslate"><span class="pre">predict_fn</span></code> which takes a raw piece of text, vectorizes it using the trained <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code>, and passes the vector into the Keras model to generate class probabilities. LIME uses <code class="docutils literal notranslate"><span class="pre">predict_fn</span></code> to query our neural
network order to learn its behavior around the provided data instance.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">predict_fn</span><span class="p">(</span><span class="n">instance</span><span class="p">):</span>
    <span class="c1"># Convert string to index</span>
    <span class="n">sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">instance</span><span class="p">)</span>

    <span class="c1"># Padding</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">)</span>

    <span class="c1"># Convert to numpy array</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">arr</span><span class="p">])</span>

<span class="n">explainer</span><span class="o">.</span><span class="n">build_explainer</span><span class="p">(</span><span class="n">predict_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-5:-Generate-some-explanations">
<h2>Step 5: Generate some explanations<a class="headerlink" href="#Step-5:-Generate-some-explanations" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">exp</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">explain_instance</span><span class="p">(</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">instance</span><span class="o">=</span><span class="n">raw_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">100</span><span class="p">],</span>
    <span class="n">num_features</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Label&#39;</span><span class="p">,</span> <span class="n">raw_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">raw_train</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="mi">100</span><span class="p">]])</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">exp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/experiments/venv_codebook_defenses/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.
  return _compile(pattern, flags).split(string, maxsplit)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Label rec.sport.baseball
{0: {&#39;confidence&#39;: 0.9999999,
     &#39;explanation&#39;: [{&#39;feature&#39;: &#39;game&#39;, &#39;score&#39;: 0.2595229194688601},
                     {&#39;feature&#39;: &#39;again&#39;, &#39;score&#39;: 0.18668745076997575},
                     {&#39;feature&#39;: &#39;Yankees&#39;, &#39;score&#39;: 0.1860972771358493},
                     {&#39;feature&#39;: &#39;pitches&#39;, &#39;score&#39;: 0.15736066007125038},
                     {&#39;feature&#39;: &#39;Liberalizer&#39;, &#39;score&#39;: 0.1347915665789044},
                     {&#39;feature&#39;: &#39;can&#39;, &#39;score&#39;: 0.12968895498952704},
                     {&#39;feature&#39;: &#39;think&#39;, &#39;score&#39;: 0.11919896484535476},
                     {&#39;feature&#39;: &#39;am&#39;, &#39;score&#39;: 0.11277455237479057},
                     {&#39;feature&#39;: &#39;I&#39;, &#39;score&#39;: -0.05480655587778587},
                     {&#39;feature&#39;: &#39;believe&#39;, &#39;score&#39;: -0.043605727115050126}]},
 1: {&#39;confidence&#39;: 2.9914535e-09,
     &#39;explanation&#39;: [{&#39;feature&#39;: &#39;game&#39;, &#39;score&#39;: -0.001329492360436989},
                     {&#39;feature&#39;: &#39;going&#39;, &#39;score&#39;: -0.0012736316324735816},
                     {&#39;feature&#39;: &#39;the&#39;, &#39;score&#39;: -0.0011308983744577278},
                     {&#39;feature&#39;: &#39;this&#39;, &#39;score&#39;: -0.0009705440718428874},
                     {&#39;feature&#39;: &#39;Allegheny&#39;, &#39;score&#39;: 0.0008826584173278725},
                     {&#39;feature&#39;: &#39;College&#39;, &#39;score&#39;: -0.0008800347780071557},
                     {&#39;feature&#39;: &#39;Yankees&#39;, &#39;score&#39;: 0.0008542046802342881},
                     {&#39;feature&#39;: &#39;can&#39;, &#39;score&#39;: 0.0007478587703458676},
                     {&#39;feature&#39;: &#39;it&#39;, &#39;score&#39;: 0.0007078390948858462},
                     {&#39;feature&#39;: &#39;believe&#39;, &#39;score&#39;: -0.0006601853762163212}]},
 2: {&#39;confidence&#39;: 7.7463795e-08,
     &#39;explanation&#39;: [{&#39;feature&#39;: &#39;game&#39;, &#39;score&#39;: -0.2580333837550864},
                     {&#39;feature&#39;: &#39;Yankees&#39;, &#39;score&#39;: -0.18679854511492092},
                     {&#39;feature&#39;: &#39;again&#39;, &#39;score&#39;: -0.1860578505206049},
                     {&#39;feature&#39;: &#39;pitches&#39;, &#39;score&#39;: -0.15752556251321137},
                     {&#39;feature&#39;: &#39;Liberalizer&#39;, &#39;score&#39;: -0.13402499305687204},
                     {&#39;feature&#39;: &#39;can&#39;, &#39;score&#39;: -0.13028440845443395},
                     {&#39;feature&#39;: &#39;think&#39;, &#39;score&#39;: -0.11928233701465708},
                     {&#39;feature&#39;: &#39;am&#39;, &#39;score&#39;: -0.11278246486555822},
                     {&#39;feature&#39;: &#39;I&#39;, &#39;score&#39;: 0.054717479221668884},
                     {&#39;feature&#39;: &#39;believe&#39;, &#39;score&#39;: 0.04448656555204294}]}}
</pre></div></div>
</div>
</div>
<div class="section" id="Step-6:-Save-and-load-the-explainer">
<h2>Step 6: Save and load the explainer<a class="headerlink" href="#Step-6:-Save-and-load-the-explainer" title="Permalink to this headline">¶</a></h2>
<p>Like with the LIME tabular explainer, we can save and load the explainer via <code class="docutils literal notranslate"><span class="pre">load_explainer</span></code> and <code class="docutils literal notranslate"><span class="pre">save_explainer</span></code> respectively.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Save the explainer somewhere</span>

<span class="n">explainer</span><span class="o">.</span><span class="n">save_explainer</span><span class="p">(</span><span class="s1">&#39;artefacts/lime_text_keras.pkl&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load the saved explainer in a new ExplainerFactory instance</span>

<span class="n">new_explainer</span> <span class="o">=</span> <span class="n">ExplainerFactory</span><span class="o">.</span><span class="n">get_explainer</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="n">xai</span><span class="o">.</span><span class="n">DOMAIN</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">xai</span><span class="o">.</span><span class="n">ALG</span><span class="o">.</span><span class="n">LIME</span><span class="p">)</span>
<span class="n">new_explainer</span><span class="o">.</span><span class="n">load_explainer</span><span class="p">(</span><span class="s1">&#39;artefacts/lime_text_keras.pkl&#39;</span><span class="p">)</span>

<span class="n">exp</span> <span class="o">=</span> <span class="n">new_explainer</span><span class="o">.</span><span class="n">explain_instance</span><span class="p">(</span>
    <span class="n">instance</span><span class="o">=</span><span class="n">raw_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">20</span><span class="p">],</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">num_features</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Label&#39;</span><span class="p">,</span> <span class="n">raw_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">raw_train</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="mi">20</span><span class="p">]])</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">exp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/experiments/venv_codebook_defenses/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.
  return _compile(pattern, flags).split(string, maxsplit)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Label rec.sport.baseball
{0: {&#39;confidence&#39;: 1.0,
     &#39;explanation&#39;: [{&#39;feature&#39;: &#39;baseball&#39;, &#39;score&#39;: 0.20855838996714443},
                     {&#39;feature&#39;: &#39;stadium&#39;, &#39;score&#39;: 0.13433699819432926},
                     {&#39;feature&#39;: &#39;football&#39;, &#39;score&#39;: 0.07024692251378775},
                     {&#39;feature&#39;: &#39;in&#39;, &#39;score&#39;: -0.031260284138860533},
                     {&#39;feature&#39;: &#39;with&#39;, &#39;score&#39;: -0.03063213505227814}]},
 1: {&#39;confidence&#39;: 3.4470056e-13,
     &#39;explanation&#39;: [{&#39;feature&#39;: &#39;baseball&#39;, &#39;score&#39;: -0.009481833034551843},
                     {&#39;feature&#39;: &#39;the&#39;, &#39;score&#39;: -0.007167239192790489},
                     {&#39;feature&#39;: &#39;multipurpose&#39;, &#39;score&#39;: 0.004503424932271883},
                     {&#39;feature&#39;: &#39;It&#39;, &#39;score&#39;: 0.004496400507397244},
                     {&#39;feature&#39;: &#39;let&#39;, &#39;score&#39;: 0.004477692674225241}]},
 2: {&#39;confidence&#39;: 3.5067134e-14,
     &#39;explanation&#39;: [{&#39;feature&#39;: &#39;baseball&#39;, &#39;score&#39;: -0.1942792635465446},
                     {&#39;feature&#39;: &#39;stadium&#39;, &#39;score&#39;: -0.12629712306511712},
                     {&#39;feature&#39;: &#39;football&#39;, &#39;score&#39;: -0.06171824737116527},
                     {&#39;feature&#39;: &#39;with&#39;, &#39;score&#39;: 0.0384714317863913},
                     {&#39;feature&#39;: &#39;play&#39;, &#39;score&#39;: -0.030882082530022472}]}}
</pre></div></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tutorial_shap_tabular_explainer.html" class="btn btn-neutral float-right" title="SHAP Kernel Explainer for Tabular Data via Contextual AI" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tutorial_lime_text_explainer.html" class="btn btn-neutral float-left" title="LIME Text Explainer via XAI" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Wang Jin, Sean Saito, Chai Wei Tah, Ni Peng, Shu Zhen, Karthik Muthuswamy, Amrit Raj

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>