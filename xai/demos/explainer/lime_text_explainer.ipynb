{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME Text Explainer via XAI\n",
    "\n",
    "This tutorial demonstrates how to generate explanations using LIME's text explainer implemented by the XAI library. Mucb of the tutorial overlaps with what is covered in the [LIME tabular tutorial](lime_tabular_explainer.ipynb). To recap, the main steps for generating explanations are:\n",
    "\n",
    "1. Instantiate the Explainer class\n",
    "2. Build the text explainer\n",
    "3. Call `explain_instance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i330688/venv_xai/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/i330688/venv_xai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Some auxiliary imports for the tutorial\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "\n",
    "# Set the path so that we can import the Explainer\n",
    "sys.path.append('../../../')\n",
    "\n",
    "# Main XAI imports\n",
    "import xai\n",
    "from xai.explainer import Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load dataset and train a model\n",
    "\n",
    "In this tutorial, we rely on the 20newsgroups text dataset, which can be loaded via sklearn's dataset utility. Documentation on the dataset itself can be found [here](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html). To keep things simple, we will extract data for 3 topics - baseball, Christianity, and medicine.\n",
    "\n",
    "Our target model is a multinomial Naive Bayes classifier, which we train using TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'filenames', 'target_names', 'target', 'DESCR', 'description']\n",
      "['rec.sport.baseball', 'sci.med', 'soc.religion.christian']\n",
      "[1 0 2 2 0 2 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9689336691855583"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on a subset of categories\n",
    "\n",
    "categories = [\n",
    "    'rec.sport.baseball',\n",
    "    'soc.religion.christian',\n",
    "    'sci.med'\n",
    "]\n",
    "\n",
    "raw_train = datasets.fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(list(raw_train.keys()))\n",
    "print(raw_train.target_names)\n",
    "print(raw_train.target[:10])\n",
    "raw_test = datasets.fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(raw_train.data)\n",
    "y_train = raw_train.target\n",
    "\n",
    "X_test = vectorizer.transform(raw_test.data)\n",
    "y_test = raw_test.target\n",
    "\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Instantiate the explainer\n",
    "\n",
    "Here, we will use the LIME Text Explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainer(domain=xai.DOMAIN.TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the explainer\n",
    "\n",
    "This initializes the underlying explainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.build_explainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate some explanations\n",
    "\n",
    "We provide the `explain_instance` method with the raw text - LIME's text explainer algorithm will conduct its own preprocessing in order to generate interpretable representations of the data. Hence we must define a custom `predict_fn` which takes a raw piece of text, vectorizes it via a pre-trained TF-IDF vectorizer, and passes the vector into the trained Naive Bayes model to generate class probabilities. LIME uses `predict_fn` to query our Naive Bayes model in order to learn its behavior around the provided data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i330688/venv_xai/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label rec.sport.baseball\n",
      "{0: {'confidence': 0.9604247937223921,\n",
      "     'explanation': [{'feature': 'Mattingly', 'score': 0.15948847368512384},\n",
      "                     {'feature': 'njit', 'score': -0.058093742192877086},\n",
      "                     {'feature': 'tesla', 'score': -0.05226657434448741},\n",
      "                     {'feature': 'luriem', 'score': 0.041491210763013854},\n",
      "                     {'feature': 'Yankee', 'score': 0.040511388496675634},\n",
      "                     {'feature': 'Lurie', 'score': 0.03945369764822541},\n",
      "                     {'feature': 'Allegheny', 'score': 0.038682356052868946},\n",
      "                     {'feature': 'PLAYERS', 'score': 0.038581528968433275},\n",
      "                     {'feature': 'Liberalizer', 'score': 0.03748895841638309},\n",
      "                     {'feature': 'game', 'score': 0.03528869693865674}]},\n",
      " 1: {'confidence': 0.015984823571617023,\n",
      "     'explanation': [{'feature': 'Mattingly', 'score': -0.05353213121137752},\n",
      "                     {'feature': 'game', 'score': -0.020368749602553103},\n",
      "                     {'feature': 'alleg', 'score': -0.020334778105712815},\n",
      "                     {'feature': 'luriem', 'score': -0.020276881922028343},\n",
      "                     {'feature': 'Allegheny', 'score': -0.020143796500731833},\n",
      "                     {'feature': 'Lurie', 'score': -0.01942905163460649},\n",
      "                     {'feature': 'tesla', 'score': 0.019013647815547163},\n",
      "                     {'feature': '1993Apr21', 'score': 0.014765042470528416},\n",
      "                     {'feature': 'Don', 'score': 0.011774298505177761},\n",
      "                     {'feature': 'njit', 'score': 0.011059561577016867}]},\n",
      " 2: {'confidence': 0.02359038270598772,\n",
      "     'explanation': [{'feature': 'Mattingly', 'score': -0.11742256976014134},\n",
      "                     {'feature': 'njit', 'score': 0.03813525649028631},\n",
      "                     {'feature': 'Lurie', 'score': -0.03323516570040962},\n",
      "                     {'feature': 'Yankee', 'score': -0.033187550975475176},\n",
      "                     {'feature': 'luriem', 'score': -0.03314282989117507},\n",
      "                     {'feature': 'PLAYERS', 'score': -0.03249877037465244},\n",
      "                     {'feature': 'Liberalizer', 'score': -0.03218640861369756},\n",
      "                     {'feature': 'Jesus', 'score': 0.025710687209416227},\n",
      "                     {'feature': 'christ', 'score': 0.02400315821692325},\n",
      "                     {'feature': 'tesla', 'score': 0.022378035919519087}]}}\n"
     ]
    }
   ],
   "source": [
    "def predict_fn(instance):\n",
    "    vec = vectorizer.transform(instance)\n",
    "    return clf.predict_proba(vec)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    predict_fn=predict_fn,\n",
    "    labels=[0, 1, 2],\n",
    "    instance=raw_test.data[0],\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "print('Label', raw_train.target_names[raw_test.target[0]])\n",
    "pprint(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the LIME tabular explainer, the output of `explain_instance` is a JSON-compatible object where each class index maps to the target model's confidence and the corresponding explanations generated by LIME. For text, each feature is a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label sci.med\n",
      "{0: {'confidence': 0.006374625691451515,\n",
      "     'explanation': [{'feature': 'pain', 'score': -0.02740261143993559},\n",
      "                     {'feature': 'sr', 'score': 0.02617688083387586},\n",
      "                     {'feature': 'ai', 'score': -0.02391983644002593},\n",
      "                     {'feature': 'Covington', 'score': -0.020875042515066302},\n",
      "                     {'feature': 'mcovingt', 'score': -0.020699977679627755}]},\n",
      " 1: {'confidence': 0.8824748491424798,\n",
      "     'explanation': [{'feature': 'hp', 'score': 0.06962985800565993},\n",
      "                     {'feature': 'doctor', 'score': 0.067793107925725},\n",
      "                     {'feature': 'pain', 'score': 0.06680102769302987},\n",
      "                     {'feature': 'kidney', 'score': 0.05490790579203543},\n",
      "                     {'feature': 'Kidney', 'score': 0.05326854053175153}]},\n",
      " 2: {'confidence': 0.11115052516607107,\n",
      "     'explanation': [{'feature': 'hp', 'score': -0.07999974792513226},\n",
      "                     {'feature': 'doctor', 'score': -0.04754155417624491},\n",
      "                     {'feature': 'pain', 'score': -0.04122731974890105},\n",
      "                     {'feature': 'kidney', 'score': -0.0395055004527884},\n",
      "                     {'feature': 'Kidney', 'score': -0.03753117417614438}]}}\n"
     ]
    }
   ],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    predict_fn=predict_fn,\n",
    "    labels=[0, 1, 2],\n",
    "    instance=raw_test.data[7],\n",
    "    num_features=5\n",
    ")\n",
    "\n",
    "print('Label', raw_train.target_names[raw_test.target[7]])\n",
    "pprint(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label soc.religion.christian\n",
      "{0: {'confidence': 6.798212345437472e-05,\n",
      "     'explanation': [{'feature': 'Bible', 'score': -0.002350080976348548},\n",
      "                     {'feature': 'Scripture', 'score': -0.001434457771521199},\n",
      "                     {'feature': 'Heaven', 'score': -0.0013811963568868962},\n",
      "                     {'feature': 'Sin', 'score': -0.00137237244087949},\n",
      "                     {'feature': 'specific', 'score': -0.0013611914394935853}]},\n",
      " 1: {'confidence': 0.00044272540371258136,\n",
      "     'explanation': [{'feature': 'Bible', 'score': -0.007407412195931128},\n",
      "                     {'feature': 'Scripture', 'score': -0.00365836775767881},\n",
      "                     {'feature': 'Heaven', 'score': -0.003652181996607399},\n",
      "                     {'feature': 'immoral', 'score': -0.003469502264458388},\n",
      "                     {'feature': 'Sin', 'score': -0.003246609821338069}]},\n",
      " 2: {'confidence': 0.9994892924728337,\n",
      "     'explanation': [{'feature': 'Bible', 'score': 0.00973653997148663},\n",
      "                     {'feature': 'Scripture', 'score': 0.00512437563602413},\n",
      "                     {'feature': 'Heaven', 'score': 0.005053514624616302},\n",
      "                     {'feature': 'immoral', 'score': 0.004781252244149244},\n",
      "                     {'feature': 'Sin', 'score': 0.004596128058053573}]}}\n"
     ]
    }
   ],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    predict_fn=predict_fn,\n",
    "    labels=[0, 1, 2],\n",
    "    instance=raw_test.data[9],\n",
    "    num_features=5\n",
    ")\n",
    "\n",
    "print('Label', raw_train.target_names[raw_test.target[9]])\n",
    "pprint(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save and load the explainer\n",
    "\n",
    "Like with the LIME tabular explainer, we can save and load the explainer via `load_explainer` and `save_explainer` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the explainer somewhere\n",
    "\n",
    "explainer.save_explainer('artefacts/lime_text.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i330688/venv_xai/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label soc.religion.christian\n",
      "{0: {'confidence': 6.798212345437472e-05,\n",
      "     'explanation': [{'feature': 'Bible', 'score': -0.002393317095822573},\n",
      "                     {'feature': 'eternal', 'score': -0.0013269313594894295},\n",
      "                     {'feature': 'Sin', 'score': -0.0013264176687192157},\n",
      "                     {'feature': 'Scripture', 'score': -0.0013192950166291022},\n",
      "                     {'feature': 'logical', 'score': -0.0012341366536975005}]},\n",
      " 1: {'confidence': 0.00044272540371258136,\n",
      "     'explanation': [{'feature': 'Bible', 'score': -0.007634815018552455},\n",
      "                     {'feature': 'Scripture', 'score': -0.0034759122077137384},\n",
      "                     {'feature': 'Sin', 'score': -0.0033190824276768936},\n",
      "                     {'feature': 'immoral', 'score': -0.003251004838219847},\n",
      "                     {'feature': 'Heaven', 'score': -0.003229818320004681}]},\n",
      " 2: {'confidence': 0.9994892924728337,\n",
      "     'explanation': [{'feature': 'Bible', 'score': 0.01007923054519195},\n",
      "                     {'feature': 'Scripture', 'score': 0.004752339637588284},\n",
      "                     {'feature': 'eternal', 'score': 0.004605293699504958},\n",
      "                     {'feature': 'Sin', 'score': 0.004594037307237789},\n",
      "                     {'feature': 'Heaven', 'score': 0.004529884511827236}]}}\n"
     ]
    }
   ],
   "source": [
    "# Load the saved explainer in a new Explainer instance\n",
    "\n",
    "new_explainer = Explainer(domain=xai.DOMAIN.TEXT, algorithm=xai.ALG.LIME)\n",
    "new_explainer.load_explainer('artefacts/lime_text.pkl')\n",
    "\n",
    "exp = new_explainer.explain_instance(\n",
    "    predict_fn=predict_fn,\n",
    "    labels=[0, 1, 2],\n",
    "    instance=raw_test.data[9],\n",
    "    num_features=5\n",
    ")\n",
    "\n",
    "print('Label', raw_train.target_names[raw_test.target[9]])\n",
    "pprint(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
