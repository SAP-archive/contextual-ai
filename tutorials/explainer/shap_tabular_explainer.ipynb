{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Kernel Explainer for Tabular Data via Contextual AI\n",
    "\n",
    "This tutorial demonstrates how to generate explanations using SHAP's Kernel Explainer implemented by the Contextual AI library. Much of the tutorial overlaps with what is covered in the [LIME tabular tutorial](lime_tabular_explainer.ipynb). To recap, the main steps for generating explanations are:\n",
    "\n",
    "1. Get an explainer via the `ExplainerFactory` class\n",
    "2. Build the text explainer\n",
    "3. Call `explain_instance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some auxiliary imports for the tutorial\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import os\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "\n",
    "# Set the path so that we can import ExplainerFactory\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Main Contextual AI imports\n",
    "import xai\n",
    "from xai.explainer import ExplainerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train a model on a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i330688/venv_xai/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset and prepare training and test sets\n",
    "raw_data = datasets.load_breast_cancer()\n",
    "X, y = raw_data['data'], raw_data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate a classifier, train, and evaluate on test set\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Instantiate the explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LimeTabularExplainer via the ExplainerFactory interface\n",
    "explainer = ExplainerFactory.get_explainer(domain=xai.DOMAIN.TABULAR, algorithm=xai.ALG.SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the explainer\n",
    "\n",
    "Like with any explainer in Contextual AI, the SHAP Kernel Explainer implements a `build_explainer` method to initialize the explainer (this can include pre-training a model or initializing some parameters). Note, however, that the `build_explainer` for SHAP requires a different set of parameters than that of the LIME Tabular Explainer. This also goes for `explain_instance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 455 background data samples could cause slower run times. Consider using shap.kmeans(data, K) to summarize the background as K weighted samples.\n"
     ]
    }
   ],
   "source": [
    "explainer.build_explainer(\n",
    "    predict_fn=clf.predict_proba,\n",
    "    training_data=X_train,\n",
    "    feature_names=raw_data['feature_names']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate some explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{0: {'confidence': 0.0,\n",
      "     'explanation': [{'feature': 'mean texture = 11.97',\n",
      "                      'score': -0.05705665528004522},\n",
      "                     {'feature': 'mean area = 288.5',\n",
      "                      'score': -0.059909773253767146},\n",
      "                     {'feature': 'worst radius = 10.62',\n",
      "                      'score': -0.06309303920038445},\n",
      "                     {'feature': 'worst perimeter = 66.53',\n",
      "                      'score': -0.10050683860714402},\n",
      "                     {'feature': 'worst area = 342.9',\n",
      "                      'score': -0.08910402332898809}]},\n",
      " 1: {'confidence': 1.0,\n",
      "     'explanation': [{'feature': 'mean texture = 11.97',\n",
      "                      'score': 0.057056655280045665},\n",
      "                     {'feature': 'mean area = 288.5',\n",
      "                      'score': 0.05990977325376773},\n",
      "                     {'feature': 'worst radius = 10.62',\n",
      "                      'score': 0.06309303920038489},\n",
      "                     {'feature': 'worst perimeter = 66.53',\n",
      "                      'score': 0.10050683860714457},\n",
      "                     {'feature': 'worst area = 342.9',\n",
      "                      'score': 0.08910402332898854}]}}\n"
     ]
    }
   ],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    instance=X_test[0],\n",
    "    num_samples=None,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "pprint(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save and load the explainer\n",
    "\n",
    "Any explanation algorithm in Contextual AI can be saved/loaded via `save_explainer` and `load_explainer`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the explainer somewhere\n",
    "\n",
    "explainer.save_explainer('artefacts/shap_tabular.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{0: {'confidence': 0.0,\n",
      "     'explanation': [{'feature': 'mean texture = 11.97',\n",
      "                      'score': -0.05275998109397245},\n",
      "                     {'feature': 'mean area = 288.5',\n",
      "                      'score': -0.06533435495159351},\n",
      "                     {'feature': 'worst radius = 10.62',\n",
      "                      'score': -0.06228954336585418},\n",
      "                     {'feature': 'worst perimeter = 66.53',\n",
      "                      'score': -0.09468562012851084},\n",
      "                     {'feature': 'worst area = 342.9',\n",
      "                      'score': -0.09460083013039794}]},\n",
      " 1: {'confidence': 1.0,\n",
      "     'explanation': [{'feature': 'mean texture = 11.97',\n",
      "                      'score': 0.05275998109397295},\n",
      "                     {'feature': 'mean area = 288.5',\n",
      "                      'score': 0.06533435495159409},\n",
      "                     {'feature': 'worst radius = 10.62',\n",
      "                      'score': 0.062289543365854405},\n",
      "                     {'feature': 'worst perimeter = 66.53',\n",
      "                      'score': 0.09468562012851142},\n",
      "                     {'feature': 'worst area = 342.9',\n",
      "                      'score': 0.0946008301303985}]}}\n"
     ]
    }
   ],
   "source": [
    "# Load the saved explainer in a new Explainer instance\n",
    "\n",
    "new_explainer = ExplainerFactory.get_explainer(domain=xai.DOMAIN.TABULAR, algorithm=xai.ALG.SHAP)\n",
    "new_explainer.load_explainer('artefacts/shap_tabular.pkl')\n",
    "\n",
    "exp = new_explainer.explain_instance(\n",
    "    instance=X_test[0],\n",
    "    num_samples=None,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "pprint(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SHAP model is pretty large, so remove it from disk\n",
    "os.remove('artefacts/shap_tabular.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
